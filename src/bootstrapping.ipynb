{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import graphviz\n",
    "import lime\n",
    "import xgboost as xgb\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Pipeline for train.csv\n",
    "#############################################################\n",
    "def pipeline(data):\n",
    "    # Drop empty columns\n",
    "    garbage = ['F25', 'F26', 'F27']\n",
    "    data.drop(garbage, axis=1, inplace=True)\n",
    "    \n",
    "    # Drop columns with very low correlation to label\n",
    "    low_corr = ['F20', 'F23', 'F21', 'F18', 'F1', 'F24', \n",
    "            'F11', 'F13', 'F2', 'F15', 'F8', 'F14', 'F22']\n",
    "    data.drop(low_corr, axis=1, inplace=True)\n",
    "    data.drop(['id'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop duplicate columns\n",
    "    dups = ['F9', 'F12']\n",
    "    data.drop(dups, axis=1, inplace=True)\n",
    "\n",
    "    # F6\n",
    "    for i in range(10):\n",
    "        data_point = data['F6'].idxmax()\n",
    "        data.drop([data_point], inplace=True)\n",
    "    data.F6 = np.log(data.F6)\n",
    "\n",
    "    # F16\n",
    "    data = data[data['F16'] > 115000]\n",
    "    data.F16 -= data.F16.min()\n",
    "    data.F16 /= m.sqrt(data.F16.std())\n",
    "\n",
    "    # F20\n",
    "    #data = data[data.F20 != 12]\n",
    "    \n",
    "    # F3\n",
    "    data.F3 += 1\n",
    "    data.F3 = np.log(data.F3)\n",
    "    \n",
    "    # F4\n",
    "    data = zeroMean(data, 'F4')\n",
    "    \n",
    "    # F5\n",
    "    data = data[data.F5 < 180000]\n",
    "    data.F5 -= data.F5.min()\n",
    "    data.F5 /= m.sqrt(data.F5.std())\n",
    "    \n",
    "    # F7\n",
    "    column = 'F7'\n",
    "    data.loc[data[column] < 75000, column] = 1\n",
    "    data.loc[(data[column] < 215000) & (data[column] > 2), column] = 2\n",
    "    data.loc[data[column] > 215000, column] = 3\n",
    "    \n",
    "    # F10\n",
    "    column = 'F10'\n",
    "    data = data[data[column] < 200000]\n",
    "    data = data[data[column] > 120000]\n",
    "    data.F10 -= data.F10.min()\n",
    "    data.F10 /= m.sqrt(data.F10.std())\n",
    "    \n",
    "    # F17\n",
    "    column = 'F17'\n",
    "    data.F17 -= data.F17.min()\n",
    "    data.F17 /= m.sqrt(data[column].std())\n",
    "    \n",
    "    # F19\n",
    "    data = data[data.F19 < 300000]\n",
    "    data.F19 /= m.sqrt(data.F19.std())\n",
    "    \n",
    "    return data\n",
    "\n",
    "#############################################################\n",
    "# Pipeline for test.csv\n",
    "#############################################################\n",
    "def testPipeline(data):\n",
    "     # Drop columns with very low correlation to label\n",
    "    low_corr = ['F20', 'F23', 'F21', 'F18', 'F1', 'F24', \n",
    "            'F11', 'F13', 'F2', 'F15', 'F8', 'F14', 'F22']\n",
    "    data.drop(low_corr, axis=1, inplace=True)\n",
    "    data.drop(['id'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop duplicate columns\n",
    "    dups = ['F9', 'F12']\n",
    "    data.drop(dups, axis=1, inplace=True)\n",
    "\n",
    "    # F6\n",
    "    data.F6 = np.log(data.F6)\n",
    "\n",
    "    # F16\n",
    "    data.F16 -= data.F16.min()\n",
    "    data.F16 /= m.sqrt(data.F16.std())\n",
    "\n",
    "    # F20\n",
    "    #data = data[data.F20 != 12]\n",
    "    \n",
    "    # F3\n",
    "    data.F3 += 1\n",
    "    data.F3 = np.log(data.F3)\n",
    "    \n",
    "    # F4\n",
    "    data = zeroMean(data, 'F4')\n",
    "    \n",
    "    # F5\n",
    "    data.F5 -= data.F5.min()\n",
    "    data.F5 /= m.sqrt(data.F5.std())\n",
    "    \n",
    "    # F7\n",
    "    column = 'F7'\n",
    "    data.loc[data[column] < 75000, column] = 1\n",
    "    data.loc[(data[column] < 215000) & (data[column] > 2), column] = 2\n",
    "    data.loc[data[column] > 215000, column] = 3\n",
    "    \n",
    "    # F10\n",
    "    data.F10 -= data.F10.min()\n",
    "    data.F10 /= m.sqrt(data.F10.std())\n",
    "    \n",
    "    # F17\n",
    "    column = 'F17'\n",
    "    data.F17 -= data.F17.min()\n",
    "    data.F17 /= m.sqrt(data[column].std())\n",
    "    \n",
    "    # F19\n",
    "    data.F19 /= m.sqrt(data.F19.std())\n",
    "    \n",
    "    return data\n",
    "\n",
    "#############################################################\n",
    "# Writes a file for Kaggle Submission\n",
    "#############################################################\n",
    "def makeFile(pred, filename):\n",
    "    new_index = np.arange(16384,32769,1)\n",
    "    id_col = pd.DataFrame(new_index, columns=['id'], dtype='int32')\n",
    "    y_hat = pd.DataFrame(pred, columns=['Y'])\n",
    "    frames = [id_col, y_hat]\n",
    "    pred = pd.concat(frames, axis=1)\n",
    "    pred.to_csv(filename, encoding='utf-8', index=False)\n",
    "\n",
    "def zeroMean(data, column):\n",
    "    data[column] -= data[column].mean()\n",
    "    data[column] /= m.sqrt(data[column].std())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Bootstrapping\n",
    "I wanted to try to just bootstrap an enormous data set and see how it was able to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train.csv'\n",
    "filepath = ''\n",
    "data = pd.read_csv(filepath + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/programming/Kaggle-Midterm/lib/python3.5/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "/mnt/c/programming/Kaggle-Midterm/lib/python3.5/site-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/mnt/c/programming/Kaggle-Midterm/lib/python3.5/site-packages/ipykernel_launcher.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "new_data = pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_boot = np.zeros((10**6, new_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_boot.shape[0]):\n",
    "    #seed = np.random.randint(0,100)\n",
    "    rand = np.random.randint(0, new_data.shape[0])\n",
    "    X_boot[i][:] = new_data.iloc[rand,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = X_boot[:,0]\n",
    "features = X_boot[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for L1 norm is: 0.9414912\n",
      "St Dev for L1 norm is: 0.00041889683694198987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = [] \n",
    "weights_L1 = []\n",
    "for i in range(10):\n",
    "    rand = np.random.randint(1, 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    clf = LogisticRegression(penalty='l1', max_iter=1000, random_state=rand)\n",
    "    _ = clf.fit(X_train, y_train)\n",
    "    accuracies.append(clf.score(X_test, y_test))\n",
    "    weights_L1.append(clf.coef_)\n",
    "\n",
    "accuracies = np.array(accuracies)\n",
    "print(\"Mean for L1 norm is: {}\".format(np.mean(accuracies, axis=0)))\n",
    "print(\"St Dev for L1 norm is: {}\".format(np.std(accuracies, axis=0)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = getTestFeatures()\n",
    "preds = clf.predict(test_features)\n",
    "pred = makeSubmission(preds)\n",
    "filename = 'prediction_bootstrap_log.csv'\n",
    "pred.to_csv(filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression doesnt seem to work very well ever on this data set. I'm going to retire it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Y', 'F3', 'F4', 'F5', 'F6', 'F7', 'F10', 'F16', 'F17', 'F19'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(loss='exponential', learning_rate=0.1, \n",
    "                                 n_estimators=500, max_depth=6, subsample=0.5)\n",
    "_ = clf.fit(features, labels)\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data = testPipeline(test_data)\n",
    "pred = clf.predict(test_data)\n",
    "makeFile(pred, 'prediction-pipelined-bootstrapped-gradboost.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
